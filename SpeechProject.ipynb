{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5Ovv7+7E5CD7rH8IBPhyF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShehabOrban/shehab1/blob/main/SpeechProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psOpHWYtp5I2",
        "outputId": "657e243d-2906-48cf-f4b9-473272996b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.1.0 rapidfuzz-3.13.0\n",
            "Downloading data from https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "\u001b[1m2748572632/2748572632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 0us/step\n",
            "Size of the training set: 11790\n",
            "Size of the training set: 1310\n",
            "The vocabulary is: ['', np.str_('a'), np.str_('b'), np.str_('c'), np.str_('d'), np.str_('e'), np.str_('f'), np.str_('g'), np.str_('h'), np.str_('i'), np.str_('j'), np.str_('k'), np.str_('l'), np.str_('m'), np.str_('n'), np.str_('o'), np.str_('p'), np.str_('q'), np.str_('r'), np.str_('s'), np.str_('t'), np.str_('u'), np.str_('v'), np.str_('w'), np.str_('x'), np.str_('y'), np.str_('z'), np.str_(\"'\"), np.str_('?'), np.str_('!'), np.str_(' ')] (size =31)\n"
          ]
        }
      ],
      "source": [
        "!pip install jiwer\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from jiwer import wer\n",
        "\n",
        "import keras\n",
        "import pandas as pd\n",
        "\n",
        "data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
        "data_path = keras.utils.get_file(\"LJSpeech-1.1\", data_url, untar=True)\n",
        "wavs_path = data_path + \"/LJSpeech-1.1/wavs/\"\n",
        "metadata_path = data_path + \"/LJSpeech-1.1/metadata.csv\"\n",
        "\n",
        "# Read metadata file and parse it\n",
        "metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\n",
        "metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n",
        "metadata_df = metadata_df[[\"file_name\", \"normalized_transcription\"]]\n",
        "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
        "metadata_df.head(3)\n",
        "\n",
        "split = int(len(metadata_df) * 0.90)\n",
        "df_train = metadata_df[:split]\n",
        "df_val = metadata_df[split:]\n",
        "\n",
        "print(f\"Size of the training set: {len(df_train)}\")\n",
        "print(f\"Size of the training set: {len(df_val)}\")\n",
        "\n",
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")\n",
        "\n",
        "# An integer scalar Tensor. The window length in samples.\n",
        "frame_length = 256\n",
        "# An integer scalar Tensor. The number of samples to step.\n",
        "frame_step = 160\n",
        "# An integer scalar Tensor. The size of the FFT to apply.\n",
        "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
        "fft_length = 384\n",
        "\n",
        "\n",
        "def encode_single_sample(wav_file, label):\n",
        "    ###########################################\n",
        "    ##  Process the Audio\n",
        "    ##########################################\n",
        "    # 1. Read wav file\n",
        "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
        "    # 2. Decode the wav file\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    # 3. Change type to float\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    # 4. Get the spectrogram\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    # 6. normalisation\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "    ###########################################\n",
        "    ##  Process the label\n",
        "    ##########################################\n",
        "    # 7. Convert label to Lower case\n",
        "    label = tf.strings.lower(label)\n",
        "    # 8. Split the label\n",
        "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "    # 9. Map the characters in label to numbers\n",
        "    label = char_to_num(label)\n",
        "    # 10. Return a dict as our model is expecting two inputs\n",
        "    return spectrogram, label\n",
        "\n",
        "batch_size = 32\n",
        "# Define the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n",
        ")\n",
        "train_dataset = (\n",
        "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Define the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_val[\"file_name\"]), list(df_val[\"normalized_transcription\"]))\n",
        ")\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the Transformer Encoder**"
      ],
      "metadata": {
        "id": "yUMl2xMFq8TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow jiwer matplotlib pandas numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3VLiVW3asTc4",
        "outputId": "24e20f39-f57c-4024-8233-5d79e66f02e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input shape parameters for the model\n",
        "def CTCLoss(y_true, y_pred):\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "t7q22glPqGxs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, num_vocab=1000, maxlen=100, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x)\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n"
      ],
      "metadata": {
        "id": "U_MSOOxnqG_M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechFeatureEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim=192, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.conv1 = layers.Conv1D(\n",
        "            embed_dim, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv2 = layers.Conv1D(\n",
        "            embed_dim, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv3 = layers.Conv1D(\n",
        "            embed_dim, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.pos_emb = layers.Embedding(input_dim=2048, output_dim=embed_dim)\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = x + positions\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "QWbPztudqHSE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "metadata": {
        "id": "-rv3ehdXqX--"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.self_att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.enc_att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "        self.dropout3 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "        \"\"\"Masks the upper half of the dot product matrix in self attention\"\"\"\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j - n_src + n_dest\n",
        "        mask = tf.cast(m, dtype)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, enc_output, target, training=False):\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "\n",
        "        # Self-attention on decoder inputs with causal mask\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
        "        target_att = self.dropout1(target_att, training=training)\n",
        "        out1 = self.layernorm1(target + target_att)\n",
        "\n",
        "        # Attention with encoder outputs\n",
        "        enc_att = self.enc_att(out1, enc_output)\n",
        "        enc_att = self.dropout2(enc_att, training=training)\n",
        "        out2 = self.layernorm2(out1 + enc_att)\n",
        "\n",
        "        # Feed forward network\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)\n"
      ],
      "metadata": {
        "id": "YlgmJE5ZqYBc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "def build_asr_model():\n",
        "    # Input features: spectrograms\n",
        "    input_spectrogram = layers.Input((None, fft_length // 2 + 1), name=\"input\")\n",
        "\n",
        "    # Embedding for spectrogram\n",
        "    x = layers.Reshape((-1, fft_length // 2 + 1, 1))(input_spectrogram)\n",
        "    x = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "\n",
        "    # Speech feature embedding\n",
        "    x = SpeechFeatureEmbedding(embed_dim=192)(x)\n",
        "\n",
        "    # Transformer encoder blocks\n",
        "    embed_dim = 192\n",
        "    num_heads = 8\n",
        "    ff_dim = 512\n",
        "\n",
        "    encoder_output = x\n",
        "    for _ in range(4):  # 4 transformer encoder blocks\n",
        "        encoder_output = TransformerEncoder(embed_dim, num_heads, ff_dim)(encoder_output)\n",
        "\n",
        "    # CTC Head\n",
        "    ctc_output = layers.Dense(char_to_num.vocabulary_size(), activation=\"softmax\", name=\"ctc_output\")(encoder_output)\n",
        "\n",
        "    # Decoder input\n",
        "    decoder_input = layers.Input(shape=(None,), dtype=tf.int32, name=\"decoder_input\")\n",
        "    decoder_emb = TokenEmbedding(num_vocab=char_to_num.vocabulary_size(), embed_dim=embed_dim)(decoder_input)\n",
        "\n",
        "    # Transformer decoder blocks\n",
        "    decoder_output = decoder_emb\n",
        "    for _ in range(2):  # 2 transformer decoder blocks\n",
        "        decoder_output = TransformerDecoder(embed_dim, num_heads, ff_dim)(encoder_output, decoder_output)\n",
        "\n",
        "    # Decoder head\n",
        "    decoder_output = layers.Dense(char_to_num.vocabulary_size(), activation=\"softmax\", name=\"decoder_output\")(decoder_output)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(\n",
        "        inputs=[input_spectrogram, decoder_input],\n",
        "        outputs=[ctc_output, decoder_output]\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss={\n",
        "            \"ctc_output\": CTCLoss,\n",
        "            \"decoder_output\": keras.losses.SparseCategoricalCrossentropy()\n",
        "        },\n",
        "        metrics={\n",
        "            \"decoder_output\": [\"accuracy\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "0bgMFF_TqYIN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For inference (prediction), we will need a separate model\n",
        "def build_inference_model(trained_model):\n",
        "    # Encoder model for extracting features\n",
        "    encoder_input = trained_model.get_layer(\"input\").input\n",
        "    encoder_output = trained_model.get_layer(\"ctc_output\").input  # Get output before CTC layer\n",
        "    encoder_model = keras.Model(encoder_input, encoder_output)\n",
        "\n",
        "    # CTC prediction model\n",
        "    ctc_prediction = trained_model.get_layer(\"ctc_output\").output\n",
        "    ctc_model = keras.Model(encoder_input, ctc_prediction)\n",
        "\n",
        "    # Decoder model for autoregressive prediction\n",
        "    decoder_input = trained_model.get_layer(\"decoder_input\").input\n",
        "    decoder_output = trained_model.get_layer(\"decoder_output\").output\n",
        "    decoder_model = keras.Model(\n",
        "        [encoder_input, decoder_input],\n",
        "        decoder_output\n",
        "    )\n",
        "\n",
        "    return encoder_model, ctc_model, decoder_model\n"
      ],
      "metadata": {
        "id": "CzoLQmSuqYMF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate target sequences for decoder during training\n",
        "def get_decoder_input(batch_targets):\n",
        "    # Add start token (0) to the beginning of each target sequence\n",
        "    decoder_input = tf.concat(\n",
        "        [tf.ones((batch_targets.shape[0], 1), dtype=tf.int32) * char_to_num.vocabulary_size(),\n",
        "         batch_targets[:, :-1]], axis=1\n",
        "    )\n",
        "    return decoder_input\n"
      ],
      "metadata": {
        "id": "6XPjm0deqlXG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example custom training loop\n",
        "def custom_train_step(model, input_batch, target_batch, optimizer):\n",
        "    decoder_input = get_decoder_input(target_batch)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        ctc_output, decoder_output = model([input_batch, decoder_input], training=True)\n",
        "\n",
        "        # Calculate losses\n",
        "        ctc_loss = CTCLoss(target_batch, ctc_output)\n",
        "        decoder_loss = keras.losses.sparse_categorical_crossentropy(\n",
        "            target_batch, decoder_output\n",
        "        )\n",
        "\n",
        "        # Combine losses\n",
        "        total_loss = ctc_loss + decoder_loss\n",
        "\n",
        "    # Get gradients and update weights\n",
        "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return {\"ctc_loss\": ctc_loss, \"decoder_loss\": decoder_loss}\n"
      ],
      "metadata": {
        "id": "-BARKlifqlZU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement CTC decoding\n",
        "def decode_ctc_predictions(predictions):\n",
        "    input_len = tf.ones(shape=tf.shape(predictions)[0]) * tf.shape(predictions)[1]\n",
        "    # Use tf.keras.backend.ctc_decode for beam search\n",
        "    results = tf.keras.backend.ctc_decode(\n",
        "        predictions, input_length=input_len, greedy=False, beam_width=5\n",
        "    )[0][0]\n",
        "    # Convert to text\n",
        "    output_texts = []\n",
        "    for result in results:\n",
        "        indices = tf.gather(result, tf.where(tf.not_equal(result, -1)))\n",
        "        decoded = tf.strings.reduce_join(num_to_char(indices))\n",
        "        output_texts.append(decoded.numpy().decode(\"utf-8\"))\n",
        "    return output_texts\n"
      ],
      "metadata": {
        "id": "vpmy1ELRqlmM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example function for getting predictions\n",
        "def predict_example(spectrogram, model):\n",
        "    encoder_model, ctc_model, decoder_model = build_inference_model(model)\n",
        "\n",
        "    # CTC prediction (greedy)\n",
        "    ctc_preds = ctc_model.predict(tf.expand_dims(spectrogram, 0))\n",
        "    ctc_text = decode_ctc_predictions(ctc_preds)[0]\n",
        "\n",
        "    # For demonstration of decoder (not a full implementation)\n",
        "    # In practice, you'd need to handle start tokens and iterate through decoding\n",
        "    max_len = 100\n",
        "    decoder_input = tf.ones((1, 1), dtype=tf.int32) * char_to_num.vocabulary_size()  # Start token\n",
        "    encoder_output = encoder_model.predict(tf.expand_dims(spectrogram, 0))\n",
        "\n",
        "    decoded_text = \"\"\n",
        "    for i in range(max_len):\n",
        "        predictions = decoder_model.predict([tf.expand_dims(spectrogram, 0), decoder_input])\n",
        "        next_char_idx = tf.argmax(predictions[0, i, :], axis=-1)\n",
        "        if next_char_idx == char_to_num('?'):  # End token\n",
        "            break\n",
        "        char = num_to_char(next_char_idx).numpy().decode(\"utf-8\")\n",
        "        decoded_text += char\n",
        "        decoder_input = tf.concat([decoder_input, tf.ones((1, 1), dtype=tf.int32) * next_char_idx], axis=1)\n",
        "\n",
        "    return ctc_text, decoded_text"
      ],
      "metadata": {
        "id": "5EuTfcE5qs59"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement Training**"
      ],
      "metadata": {
        "id": "8MJaXGKFrQ2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = build_asr_model()\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0SNf8h2bqs8M",
        "outputId": "7b0abb1a-255f-4201-8be7-ec066941286d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m193\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m193\u001b[0m, │          \u001b[38;5;34m0\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│                     │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m193\u001b[0m, │        \u001b[38;5;34m320\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m193\u001b[0m, │      \u001b[38;5;34m9,248\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m,  │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m,  │     \u001b[38;5;34m36,928\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m3072\u001b[0m)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ speech_feature_emb… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m) │  \u001b[38;5;34m7,692,864\u001b[0m │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mSpeechFeatureEmbe…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m) │  \u001b[38;5;34m1,382,528\u001b[0m │ speech_feature_e… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m) │  \u001b[38;5;34m1,382,528\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m) │  \u001b[38;5;34m1,382,528\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m) │  \u001b[38;5;34m1,382,528\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_embedding     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m) │     \u001b[38;5;34m25,152\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mTokenEmbedding\u001b[0m)    │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m) │  \u001b[38;5;34m2,567,360\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ token_embedding[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m) │  \u001b[38;5;34m2,567,360\u001b[0m │ transformer_enco… │\n",
              "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ transformer_deco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ctc_output (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m)  │      \u001b[38;5;34m5,983\u001b[0m │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m)  │      \u001b[38;5;34m5,983\u001b[0m │ transformer_deco… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span>, │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span>, │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span>, │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ speech_feature_emb… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,692,864</span> │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpeechFeatureEmbe…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,528</span> │ speech_feature_e… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,528</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,528</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_encode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,382,528</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ token_embedding     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,152</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenEmbedding</span>)    │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,567,360</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ token_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ transformer_decode… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,567,360</span> │ transformer_enco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ transformer_deco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ ctc_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,983</span> │ transformer_enco… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,983</span> │ transformer_deco… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,459,806\u001b[0m (70.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,459,806</span> (70.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,459,806\u001b[0m (70.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,459,806</span> (70.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll need to prepare the target sequences for the decoder input\n",
        "class ASRDataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, dataset, batch_size=32):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = list(range(len(dataset)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch = list(self.dataset.take(1).as_numpy_iterator())[0]\n",
        "        spectrograms, labels = batch\n",
        "\n",
        "        # Create decoder inputs by shifting the labels\n",
        "        decoder_inputs = np.zeros_like(labels)\n",
        "        decoder_inputs[:, 1:] = labels[:, :-1]\n",
        "        decoder_inputs[:, 0] = char_to_num.vocabulary_size()  # Start token\n",
        "\n",
        "        return [spectrograms, decoder_inputs], [labels, labels]\n"
      ],
      "metadata": {
        "id": "HCYT9c_qqs-l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 1  # You might need more epochs for good performance\n",
        "learning_rate = 1e-4\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "Ds3sbwneqtDt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom training for more control\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # Training loop\n",
        "    train_loss = {\"ctc\": 0.0, \"decoder\": 0.0}\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch in train_dataset:\n",
        "        spectrograms, labels = batch\n",
        "\n",
        "        # Create decoder inputs (add start token, shift right)\n",
        "        decoder_inputs = tf.zeros_like(labels)\n",
        "        decoder_inputs = tf.concat(\n",
        "            [tf.ones((tf.shape(labels)[0], 1), dtype=tf.int64) * char_to_num.vocabulary_size(),\n",
        "             labels[:, :-1]],\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            ctc_output, decoder_output = model([spectrograms, decoder_inputs], training=True)\n",
        "\n",
        "            # Calculate CTC loss\n",
        "            batch_len = tf.shape(labels)[0]\n",
        "            input_length = tf.shape(ctc_output)[1]\n",
        "            label_length = tf.shape(labels)[1]\n",
        "\n",
        "            # Reshape to 1D tensors for ctc_loss\n",
        "            # Change dtype to \"int32\" to match tf.nn.ctc_loss requirements\n",
        "            input_length = tf.cast(input_length, dtype=\"int32\") * tf.ones(shape=(batch_len,), dtype=\"int32\")\n",
        "            label_length = tf.cast(label_length, dtype=\"int32\") * tf.ones(shape=(batch_len,), dtype=\"int32\")\n",
        "\n",
        "            # Use tf.nn.ctc_loss instead of keras.backend.ctc_batch_cost\n",
        "            ctc_loss = tf.nn.ctc_loss(\n",
        "                labels=labels,\n",
        "                logits=ctc_output,\n",
        "                label_length=label_length,\n",
        "                logit_length=input_length,\n",
        "                logits_time_major=False, # Set to False as your logits are (batch, time, features)\n",
        "                blank_index=-1 # Set to -1 for compatibility with StringLookup\n",
        "            )\n",
        "\n",
        "            # Calculate decoder loss (cross entropy)\n",
        "            decoder_loss = keras.losses.sparse_categorical_crossentropy(\n",
        "                labels, decoder_output\n",
        "            )\n",
        "\n",
        "            # Combined loss (you can adjust weights if needed)\n",
        "            total_loss = tf.reduce_mean(ctc_loss) + tf.reduce_mean(decoder_loss) # Calculate the mean of ctc_loss\n",
        "\n",
        "        # Calculate gradients and update weights\n",
        "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        # Update metrics\n",
        "        train_loss[\"ctc\"] += tf.reduce_mean(ctc_loss)\n",
        "        train_loss[\"decoder\"] += tf.reduce_mean(decoder_loss)\n",
        "        num_batches += 1\n",
        "\n",
        "    # Show epoch results\n",
        "    train_loss[\"ctc\"] /= num_batches\n",
        "    train_loss[\"decoder\"] /= num_batches\n",
        "    print(f\"Training CTC Loss: {train_loss['ctc']:.4f}\")\n",
        "    print(f\"Training Decoder Loss: {train_loss['decoder']:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    if epoch % 5 == 0:  # Every 5 epochs\n",
        "        val_loss = {\"ctc\": 0.0, \"decoder\": 0.0}\n",
        "        num_val_batches = 0\n",
        "\n",
        "        for batch in validation_dataset:\n",
        "            spectrograms, labels = batch\n",
        "\n",
        "            # Create decoder inputs\n",
        "            decoder_inputs = tf.zeros_like(labels)\n",
        "            decoder_inputs = tf.concat(\n",
        "                [tf.ones((tf.shape(labels)[0], 1), dtype=tf.int64) * char_to_num.vocabulary_size(),\n",
        "                 labels[:, :-1]],\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "            # Forward pass (no training)\n",
        "            ctc_output, decoder_output = model([spectrograms, decoder_inputs], training=False)\n",
        "\n",
        "            # Calculate CTC loss\n",
        "            batch_len = tf.shape(labels)[0]\n",
        "            input_length = tf.shape(ctc_output)[1]\n",
        "            label_length = tf.shape(labels)[1]\n",
        "\n",
        "            # Reshape to 1D tensors for ctc_loss\n",
        "            # Change dtype to \"int32\" to match tf.nn.ctc_loss requirements\n",
        "            input_length = tf.cast(input_length, dtype=\"int32\") * tf.ones(shape=(batch_len,), dtype=\"int32\")\n",
        "            label_length = tf.cast(label_length, dtype=\"int32\") * tf.ones(shape=(batch_len,), dtype=\"int32\")\n",
        "\n",
        "            # Use tf.nn.ctc_loss\n",
        "            ctc_loss = tf.nn.ctc_loss(\n",
        "                labels=labels,\n",
        "                logits=ctc_output,\n",
        "                label_length=label_length,\n",
        "                logit_length=input_length,\n",
        "                logits_time_major=False,\n",
        "                blank_index=-1\n",
        "            )\n",
        "\n",
        "            # Calculate decoder loss\n",
        "            decoder_loss = keras.losses.sparse_categorical_crossentropy(\n",
        "                labels, decoder_output\n",
        "            )\n",
        "\n",
        "            # Update metrics\n",
        "            val_loss[\"ctc\"] += tf.reduce_mean(ctc_loss)\n",
        "            val_loss[\"decoder\"] += tf.reduce_mean(decoder_loss)\n",
        "            num_val_batches += 1\n",
        "\n",
        "        val_loss[\"ctc\"] /= num_val_batches\n",
        "        val_loss[\"decoder\"] /= num_val_batches\n",
        "        print(f\"Validation CTC Loss: {val_loss['ctc']:.4f}\")\n",
        "        print(f\"Validation Decoder Loss: {val_loss['decoder']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVbrybTnrvDN",
        "outputId": "71197f56-d4c4-4aa9-d0a1-1f403dc8c5de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/1\n",
            "Training CTC Loss: 796.7677\n",
            "Training Decoder Loss: 1.4726\n",
            "Validation CTC Loss: 796.8536\n",
            "Validation Decoder Loss: 1.4127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model with the .keras extension\n",
        "model.save(\"asr_model_ctc_decoder.keras\")"
      ],
      "metadata": {
        "id": "JR21Wa-VrvFc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement Inference Code**"
      ],
      "metadata": {
        "id": "TkU207KAHi_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create inference models\n",
        "def create_inference_models(trained_model):\n",
        "    \"\"\"Create separate models for inference\"\"\"\n",
        "    # CTC inference model\n",
        "    input_spectrogram = trained_model.get_layer(\"input\").input\n",
        "    ctc_output = trained_model.get_layer(\"ctc_output\").output\n",
        "    ctc_model = keras.Model(input_spectrogram, ctc_output)\n",
        "\n",
        "    # Encoder model (extract features)\n",
        "    encoder_output = trained_model.layers[-3].output  # Get encoder output\n",
        "    encoder_model = keras.Model(input_spectrogram, encoder_output)\n",
        "\n",
        "    # Decoder model (for autoregressive inference)\n",
        "    decoder_input = keras.Input(shape=(None,), dtype=tf.int32)\n",
        "    encoder_outputs = keras.Input(shape=(None, encoder_output.shape[-1]))\n",
        "\n",
        "    # Get decoder layers from trained model\n",
        "    decoder_emb = trained_model.get_layer(\"token_embedding\")\n",
        "    decoder_layers = [layer for layer in trained_model.layers if isinstance(layer, TransformerDecoder)]\n",
        "    decoder_dense = trained_model.get_layer(\"decoder_output\")\n",
        "\n",
        "    # Build decoder inference model\n",
        "    x = decoder_emb(decoder_input)\n",
        "    for decoder_layer in decoder_layers:\n",
        "        x = decoder_layer(encoder_outputs, x)\n",
        "    output = decoder_dense(x)\n",
        "\n",
        "    decoder_model = keras.Model([encoder_outputs, decoder_input], output)\n",
        "\n",
        "    return ctc_model, encoder_model, decoder_model\n"
      ],
      "metadata": {
        "id": "XFaCXyhZrvHc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_spectrogram, encoder_model, decoder_model, max_length=100):\n",
        "    \"\"\"Decode sequence using autoregressive transformer decoder\"\"\"\n",
        "    # Get encoder output\n",
        "    encoder_output = encoder_model.predict(tf.expand_dims(input_spectrogram, 0))\n",
        "\n",
        "    # Initialize target sequence with start token\n",
        "    target_seq = np.zeros((1, 1), dtype=np.int32)\n",
        "    target_seq[0, 0] = char_to_num.vocabulary_size()  # Start token\n",
        "\n",
        "    # Collect the generated characters\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    # Autoregressive generation\n",
        "    for i in range(max_length):\n",
        "        # Get predictions\n",
        "        output_tokens = decoder_model.predict([encoder_output, target_seq])\n",
        "\n",
        "        # Sample next token (use argmax for simplicity)\n",
        "        sampled_token_idx = np.argmax(output_tokens[0, i, :])\n",
        "\n",
        "        # Exit condition: either hit max length or end token\n",
        "        if sampled_token_idx == char_to_num(\"?\") or len(decoded_sentence) > max_length:\n",
        "            break\n",
        "\n",
        "        # Update target sequence for next iteration\n",
        "        char = num_to_char(sampled_token_idx).numpy().decode(\"utf-8\")\n",
        "        decoded_sentence += char\n",
        "\n",
        "        # Update the target sequence\n",
        "        target_seq = np.concatenate([target_seq, [[sampled_token_idx]]], axis=-1)\n",
        "\n",
        "    return decoded_sentence\n"
      ],
      "metadata": {
        "id": "b_cRFSpEHyjx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_decode(logits):\n",
        "    \"\"\"Perform CTC decoding on logits\"\"\"\n",
        "    # Use beam search decoding\n",
        "    input_len = np.ones(logits.shape[0]) * logits.shape[1]\n",
        "    results = keras.backend.ctc_decode(\n",
        "        logits, input_length=input_len, greedy=False, beam_width=5\n",
        "    )[0][0]\n",
        "\n",
        "    # Convert to text\n",
        "    output_text = \"\"\n",
        "    for result in results:\n",
        "        result = tf.gather(result, tf.where(tf.not_equal(result, -1)))\n",
        "        decoded = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text += decoded\n",
        "\n",
        "    return output_text\n"
      ],
      "metadata": {
        "id": "U8N57-cqHy_Q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to use the inference models\n",
        "def predict_example(wav_file):\n",
        "    # Load and preprocess audio file similar to training pipeline\n",
        "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "\n",
        "    # Extract features\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "\n",
        "    # Load trained model (assuming it's been saved)\n",
        "    model = keras.models.load_model(\n",
        "        \"asr_model_ctc_decoder\",\n",
        "        custom_objects={\n",
        "            \"TokenEmbedding\": TokenEmbedding,\n",
        "            \"SpeechFeatureEmbedding\": SpeechFeatureEmbedding,\n",
        "            \"TransformerEncoder\": TransformerEncoder,\n",
        "            \"TransformerDecoder\": TransformerDecoder,\n",
        "            \"CTCLoss\": CTCLoss\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Create inference models\n",
        "    ctc_model, encoder_model, decoder_model = create_inference_models(model)\n",
        "\n",
        "    # Get CTC prediction\n",
        "    ctc_preds = ctc_model.predict(tf.expand_dims(spectrogram, 0))\n",
        "    ctc_text = ctc_decode(ctc_preds)\n",
        "\n",
        "    # Get decoder prediction\n",
        "    decoder_text = decode_sequence(spectrogram, encoder_model, decoder_model)\n",
        "\n",
        "    return {\n",
        "        \"ctc_prediction\": ctc_text,\n",
        "        \"decoder_prediction\": decoder_text\n",
        "    }\n"
      ],
      "metadata": {
        "id": "X0LnyLNdHzBx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example evaluation\n",
        "def evaluate_model(test_dataset=validation_dataset, num_samples=10):\n",
        "    \"\"\"Evaluate the model on test data\"\"\"\n",
        "    # Load trained model\n",
        "    model = keras.models.load_model(\n",
        "        \"asr_model_ctc_decoder\",\n",
        "        custom_objects={\n",
        "            \"TokenEmbedding\": TokenEmbedding,\n",
        "            \"SpeechFeatureEmbedding\": SpeechFeatureEmbedding,\n",
        "            \"TransformerEncoder\": TransformerEncoder,\n",
        "            \"TransformerDecoder\": TransformerDecoder,\n",
        "            \"CTCLoss\": CTCLoss\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Create inference models\n",
        "    ctc_model, encoder_model, decoder_model = create_inference_models(model)\n",
        "\n",
        "    # Track metrics\n",
        "    wer_scores = {\"ctc\": [], \"decoder\": []}\n",
        "\n",
        "    # Take a sample of test data\n",
        "    sample_data = test_dataset.take(num_samples)\n",
        "\n",
        "    for batch in sample_data.as_numpy_iterator():\n",
        "        spectrograms, labels = batch\n",
        "\n",
        "        for i in range(len(spectrograms)):\n",
        "            # Ground truth\n",
        "            label_indices = [j for j in labels[i] if j < char_to_num.vocabulary_size()]\n",
        "            ground_truth = \"\".join([num_to_char(j).numpy().decode(\"utf-8\") for j in label_indices])\n",
        "\n",
        "            # CTC prediction\n",
        "            ctc_preds = ctc_model.predict(np.expand_dims(spectrograms[i], 0))\n",
        "            ctc_text = ctc_decode(ctc_preds)\n",
        "\n",
        "            # Decoder prediction\n",
        "            decoder_text = decode_sequence(spectrograms[i], encoder_model, decoder_model)\n",
        "\n",
        "            # Calculate WER\n",
        "            wer_scores[\"ctc\"].append(wer(ground_truth, ctc_text))\n",
        "            wer_scores[\"decoder\"].append(wer(ground_truth, decoder_text))\n",
        "\n",
        "            print(f\"Sample {i+1}\")\n",
        "            print(f\"Ground truth: {ground_truth}\")\n",
        "            print(f\"CTC prediction: {ctc_text}\")\n",
        "            print(f\"Decoder prediction: {decoder_text}\")\n",
        "            print(f\"CTC WER: {wer_scores['ctc'][-1]:.4f}\")\n",
        "            print(f\"Decoder WER: {wer_scores['decoder'][-1]:.4f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    # Calculate average WER\n",
        "    avg_ctc_wer = sum(wer_scores[\"ctc\"]) / len(wer_scores[\"ctc\"])\n",
        "    avg_decoder_wer = sum(wer_scores[\"decoder\"]) / len(wer_scores[\"decoder\"])\n",
        "\n",
        "    print(f\"Average CTC WER: {avg_ctc_wer:.4f}\")\n",
        "    print(f\"Average Decoder WER: {avg_decoder_wer:.4f}\")"
      ],
      "metadata": {
        "id": "kpW0nQu5H8LR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Testing**"
      ],
      "metadata": {
        "id": "WvBBIaWYIOSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize model predictions\n",
        "def visualize_prediction(wav_file):\n",
        "    \"\"\"Visualize the waveform, spectrogram, and predictions for an audio file\"\"\"\n",
        "    # Load audio file\n",
        "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "\n",
        "    # Get ground truth from metadata\n",
        "    ground_truth = metadata_df[metadata_df['file_name'] == wav_file]['normalized_transcription'].values[0]\n",
        "\n",
        "    # Process audio for model input\n",
        "    audio_float = tf.cast(audio, tf.float32)\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio_float, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram_norm = (spectrogram - means) / (stddevs + 1e-10)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = predict_example(wav_file)\n",
        "    ctc_text = predictions[\"ctc_prediction\"]\n",
        "    decoder_text = predictions[\"decoder_prediction\"]\n",
        "\n",
        "    # Calculate WER\n",
        "    ctc_wer_value = wer(ground_truth, ctc_text)\n",
        "    decoder_wer_value = wer(ground_truth, decoder_text)\n",
        "\n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(16, 10))\n",
        "\n",
        "    # Plot waveform\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(audio.numpy())\n",
        "    plt.title(\"Waveform\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot spectrogram\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.imshow(tf.transpose(spectrogram), aspect=\"auto\", origin=\"lower\")\n",
        "    plt.title(\"Spectrogram\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Display text results\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.axis(\"off\")\n",
        "    result_text = (\n",
        "        f\"Ground Truth: {ground_truth}\\n\\n\"\n",
        "        f\"CTC Prediction: {ctc_text}\\n\"\n",
        "        f\"CTC Word Error Rate: {ctc_wer_value:.4f}\\n\\n\"\n",
        "        f\"Decoder Prediction: {decoder_text}\\n\"\n",
        "        f\"Decoder Word Error Rate: {decoder_wer_value:.4f}\"\n",
        "    )\n",
        "    plt.text(0.1, 0.7, result_text, fontsize=12, verticalalignment=\"top\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5j9ipfqtH8RR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to track model progress during training\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot CTC loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['ctc_loss'], label='CTC Training Loss')\n",
        "    plt.plot(history['val_ctc_loss'], label='CTC Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('CTC Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot decoder loss and accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['decoder_loss'], label='Decoder Training Loss')\n",
        "    plt.plot(history['val_decoder_loss'], label='Decoder Validation Loss')\n",
        "    if 'decoder_accuracy' in history:\n",
        "        plt.plot(history['decoder_accuracy'], label='Decoder Accuracy')\n",
        "        plt.plot(history['val_decoder_accuracy'], label='Decoder Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss / Accuracy')\n",
        "    plt.title('Decoder Metrics')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "i213iKNQITXi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run comprehensive evaluation\n",
        "def comprehensive_evaluation(num_samples=20):\n",
        "    \"\"\"Run comprehensive evaluation on random samples\"\"\"\n",
        "    # Get random samples from validation set\n",
        "    val_files = df_val['file_name'].sample(num_samples).tolist()\n",
        "\n",
        "    # Track metrics\n",
        "    results = {\n",
        "        'ctc_wer': [],\n",
        "        'decoder_wer': [],\n",
        "        'combined_wer': [],\n",
        "        'processing_time': []\n",
        "    }\n",
        "\n",
        "    for i, file_name in enumerate(val_files):\n",
        "        print(f\"Processing sample {i+1}/{num_samples}: {file_name}\")\n",
        "\n",
        "        # Get ground truth\n",
        "        ground_truth = df_val[df_val['file_name'] == file_name]['normalized_transcription'].values[0]\n",
        "\n",
        "        # Time the prediction\n",
        "        start_time = time.time()\n",
        "        predictions = predict_example(file_name)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Extract predictions\n",
        "        ctc_text = predictions[\"ctc_prediction\"]\n",
        "        decoder_text = predictions[\"decoder_prediction\"]\n",
        "\n",
        "        # Simple ensemble (choose the shorter WER)\n",
        "        ctc_wer_value = wer(ground_truth, ctc_text)\n",
        "        decoder_wer_value = wer(ground_truth, decoder_text)\n",
        "        combined_text = ctc_text if ctc_wer_value < decoder_wer_value else decoder_text\n",
        "        combined_wer_value = min(ctc_wer_value, decoder_wer_value)\n",
        "\n",
        "        # Store results\n",
        "        results['ctc_wer'].append(ctc_wer_value)\n",
        "        results['decoder_wer'].append(decoder_wer_value)\n",
        "        results['combined_wer'].append(combined_wer_value)\n",
        "        results['processing_time'].append(end_time - start_time)\n",
        "\n",
        "        # Print sample results\n",
        "        print(f\"Ground truth: {ground_truth}\")\n",
        "        print(f\"CTC prediction: {ctc_text}\")\n",
        "        print(f\"CTC WER: {ctc_wer_value:.4f}\")\n",
        "        print(f\"Decoder prediction: {decoder_text}\")\n",
        "        print(f\"Decoder WER: {decoder_wer_value:.4f}\")\n",
        "        print(f\"Processing time: {end_time - start_time:.2f} seconds\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_ctc_wer = sum(results['ctc_wer']) / len(results['ctc_wer'])\n",
        "    avg_decoder_wer = sum(results['decoder_wer']) / len(results['decoder_wer'])\n",
        "    avg_combined_wer = sum(results['combined_wer']) / len(results['combined_wer'])\n",
        "    avg_time = sum(results['processing_time']) / len(results['processing_time'])\n",
        "\n",
        "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
        "    print(f\"Average CTC WER: {avg_ctc_wer:.4f}\")\n",
        "    print(f\"Average Decoder WER: {avg_decoder_wer:.4f}\")\n",
        "    print(f\"Average Combined WER: {avg_combined_wer:.4f}\")\n",
        "    print(f\"Average processing time: {avg_time:.2f} seconds per sample\")\n",
        "\n",
        "    # Plot distribution of WER\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist([results['ctc_wer'], results['decoder_wer'], results['combined_wer']],\n",
        "             bins=10, alpha=0.7, label=['CTC', 'Decoder', 'Combined'])\n",
        "    plt.xlabel('Word Error Rate')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.title('Distribution of Word Error Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "RhFYH7D9ITe5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of attention visualization\n",
        "def visualize_attention(wav_file):\n",
        "    \"\"\"Visualize the attention weights from the decoder\"\"\"\n",
        "    # Note: This requires modifying the model to output attention weights\n",
        "    # This is a placeholder function showing how it could be implemented\n",
        "\n",
        "    # For this to work, you'd need to:\n",
        "    # 1. Modify the TransformerDecoder to return attention weights\n",
        "    # 2. Create a separate model for attention visualization\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.title(\"Decoder Attention Visualization\")\n",
        "    plt.xlabel(\"Encoder Timesteps\")\n",
        "    plt.ylabel(\"Decoder Timesteps\")\n",
        "\n",
        "    # Placeholder - in a real implementation, you'd get actual attention weights\n",
        "    # attention_weights = get_attention_weights(wav_file)\n",
        "    # plt.imshow(attention_weights, aspect='auto', origin='lower')\n",
        "\n",
        "    plt.colorbar(label=\"Attention Weight\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "F9VKoD4xIThJ"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}